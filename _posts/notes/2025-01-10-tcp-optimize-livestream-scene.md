---
layout: post
title: '直播场景TCP秒开优化'
subtitle: 
date: 2025-01-10
author: Mr Chen
cover: '/assets/img/shan.jpg'
categories: Notes
tags: 
- Life
- Protocol
---


> 当前公司直播项目拨测的秒开指标远未达到预期，经过数据对比和分析，发现主要问题出在拨测节点 Player 与边缘节点之间的 Lastmile 网络质量上，由于目前 Player 的拉流协议使用的是基于 TCP 的标准协议(RTMP/HTTP-FLV/HLS等），并且 Player 是 Web 平台，不受控制，所以重点只能通过单边优化公司边缘节点与 Player 之间的 TCP 连接质量，尽量加快 TCP 的建连速度，以及关键帧的下发速度。


## 现状

公司使用静态节点和动态节点作为边缘以节省成本，且多种业务集中进行混布，其中动态节点质量较差，但成本较低，也是导致问题的关键所在，在对Linux 内核的升级和参数调整操作上一定要慎重。

**当前动态节点版本及相关配置：**

 - 内核版本：5.4.119-19-0006
 - 见下面章节


## 优化验证

### 环境准备

 #### TC 配置

 首先安装 tc 工具，系统默认不安装：

```bash
sudo apt update
sudo apt install -y iproute2

tc --version
```

**启动配置网损:**

比如配置 `lo` 网卡 50～150ms 左右的延迟，且包含 0～15% 的一个随机丢包，配置如下：

```bash
$ sudo su
$ tc qdisc add dev lo root netem delay 50ms 25ms distribution normal loss random 0% 15%

# 查看配置是否生效
$ tc qdisc show dev lo
  qdisc netem 8002: root refcnt 2 limit 1000 delay 50ms  25ms
```

**验证：**

```bash
$ ping 127.0.0.1 -i 0.2
PING 127.0.0.1 (127.0.0.1) 56(84) bytes of data.
64 bytes from 127.0.0.1: icmp_seq=1 ttl=64 time=102 ms
64 bytes from 127.0.0.1: icmp_seq=2 ttl=64 time=115 ms
64 bytes from 127.0.0.1: icmp_seq=3 ttl=64 time=175 ms
64 bytes from 127.0.0.1: icmp_seq=4 ttl=64 time=80.4 ms
64 bytes from 127.0.0.1: icmp_seq=5 ttl=64 time=55.7 ms
64 bytes from 127.0.0.1: icmp_seq=6 ttl=64 time=118 ms
64 bytes from 127.0.0.1: icmp_seq=7 ttl=64 time=95.2 ms
64 bytes from 127.0.0.1: icmp_seq=8 ttl=64 time=106 ms
64 bytes from 127.0.0.1: icmp_seq=9 ttl=64 time=113 ms
64 bytes from 127.0.0.1: icmp_seq=10 ttl=64 time=48.2 ms
64 bytes from 127.0.0.1: icmp_seq=11 ttl=64 time=108 ms
64 bytes from 127.0.0.1: icmp_seq=12 ttl=64 time=164 ms
64 bytes from 127.0.0.1: icmp_seq=13 ttl=64 time=50.0 ms
64 bytes from 127.0.0.1: icmp_seq=14 ttl=64 time=207 ms
64 bytes from 127.0.0.1: icmp_seq=15 ttl=64 time=56.3 ms
64 bytes from 127.0.0.1: icmp_seq=16 ttl=64 time=137 ms
64 bytes from 127.0.0.1: icmp_seq=17 ttl=64 time=92.5 ms
64 bytes from 127.0.0.1: icmp_seq=18 ttl=64 time=141 ms
64 bytes from 127.0.0.1: icmp_seq=19 ttl=64 time=154 ms
64 bytes from 127.0.0.1: icmp_seq=20 ttl=64 time=141 ms
--- 127.0.0.1 ping statistics ---
21 packets transmitted, 20 received, 4.7619% packet loss, time 4016ms
rtt min/avg/max/mdev = 48.201/113.013/207.390/42.338 ms, pipe 2
```

**恢复：**

```bash
# 删除配置
$ tc qdisc del dev lo root
```

**注意：** 不要无脑 copy 别人的 tc 配置命令，使用了 `channge` 而非 `add`，导致 `Error: Qdisc not found. To create specify NLM_F_CREATE flag.` 报错，还以为内核缺少 sch_netem 模块，差点重新安装完整内核。
可以通过命令：`modinfo sch_netem` 查看系统是否已经安装有 `sche_netem` 模块，如果没有就会报错，有的话会有模块信息输出：

```bash
filename:       /lib/modules/6.8.0-51-generic/kernel/net/sched/sch_netem.ko.zst
description:    Network characteristics emulator qdisc
license:        GPL
srcversion:     7631AD62974660130A36DCA
depends:
retpoline:      Y
intree:         Y
name:           sch_netem
vermagic:       6.8.0-51-generic SMP preempt mod_unload modversions
sig_id:         PKCS#7
signer:         Build time autogenerated kernel key
sig_key:        29:0D:80:5A:E0:B3:D6:D4:D4:D3:D0:EF:AB:48:F3:DB:73:58:2F:63
sig_hashalgo:   sha512
signature:      03:A4:1E:0E:CA:01:0F:58:3E:93:93:A7:25:97:FC:82:3E:4F:60:CA:
                00:84:75:DF:A3:20:F7:1B:92:9D:B1:58:6D:E2:47:92:84:83:00:FD:
                88:31:2B:84:32:FE:04:38:1F:24:FB:3E:D0:38:75:35:DB:11:C4:EE:
                19:B8:C1:1F:FE:B8:77:AF:AA:4B:DE:05:E7:5F:53:A4:79:65:B4:C7:
                1F:A1:6D:46:70:FA:26:00:5D:B1:08:F2:52:DD:2F:EF:AD:6E:B3:0C:
                E6:FA:9A:9A:C2:9D:00:E0:EA:D2:D2:FE:F1:40:3B:32:E3:09:20:27:
                CB:A9:8C:7E:0A:A7:2E:E4:F1:EE:89:3E:59:71:81:91:2F:CA:02:77:
                84:8B:68:92:E5:2F:CF:45:8E:31:5C:BD:54:99:0D:3E:72:55:85:79:
                A8:D6:A5:8E:EA:9A:32:21:1D:EB:28:4F:C4:54:36:1F:33:EC:82:61:
                D9:0A:DC:B7:CD:17:9B:A5:1A:89:78:94:93:42:6D:25:29:A0:E8:CD:
                7E:22:C6:BE:2F:7E:1F:E6:06:43:E3:33:2E:6F:19:1B:E8:22:45:BB:
                FC:F2:C2:A3:2D:31:F6:A3:80:19:8A:64:54:7A:BF:9C:BE:A1:D6:1B:
                2F:85:E7:F0:45:90:BC:52:42:4B:DE:5E:B5:DE:9A:1E:92:F7:F5:B3:
                9D:D5:52:03:C6:DC:2E:43:F2:4E:C6:3E:10:A0:F8:4F:24:99:66:06:
                A5:B8:48:3C:2D:AF:41:D7:84:F4:2A:61:66:3A:D2:6F:13:EA:67:5F:
                A5:DF:D3:36:F7:94:0C:85:AD:6E:CA:D7:56:E2:22:CD:AF:8A:33:25:
                41:86:10:A3:DC:47:56:DF:00:67:2D:DE:2D:66:B8:21:DF:4B:E5:B0:
                62:CC:44:7D:2F:49:C4:FD:BE:ED:35:98:C8:7E:2E:8B:27:62:32:4E:
                3A:8C:24:A9:CF:3F:69:FC:08:F9:D6:55:B1:2D:56:FB:27:8E:71:BB:
                7A:20:1C:45:77:0E:B7:83:B9:82:A4:F9:BD:C4:F1:C7:0F:6B:FB:B0:
                3D:A9:AC:A6:4B:F2:2D:AA:E0:8B:89:84:9E:35:9A:EB:C3:8A:B8:53:
                4D:A5:B3:AD:8A:2C:28:41:E7:EC:CA:46:3F:03:1C:D2:C3:EA:4E:1B:
                05:24:07:9D:A7:8A:67:58:02:9B:69:3C:ED:27:A2:33:44:4F:9A:DD:
                3D:99:D5:20:A2:AA:31:9E:9A:00:A2:F9:63:7A:F7:1A:C9:4F:9A:A7:
                F9:26:F4:F6:0B:38:BF:8A:7B:75:2B:B1:37:6C:12:4B:A0:01:E8:29:
                E0:CF:97:54:EA:1D:ED:B9:1C:D2:6F:A8
```

#### NetPerf

- 启动 netserver

启动 `netserver`, `netserver` 与 `netperf` 是同一套 Tools，只是 server 测启动命令：

```bash
$ ~$ netserver -h

Usage: netserver [options]

Options:
    -h                Display this text
    -D                Do not daemonize
    -d                Increase debugging output
    -f                Do not spawn chilren for each test, run serially
    -L name,family    Use name to pick listen address and family for family
    -N                No debugging output, even if netperf asks
    -p portnum        Listen for connect requests on portnum.
    -4                Do IPv4
    -6                Do IPv6
    -v verbosity      Specify the verbosity level
    -V                Display version information and exit
    -Z passphrase     Expect passphrase as the first thing received


# 启动命令
$ ~$ sudo netserver -p 1234 -D -4
check_if_inetd: enter
setup_listens: enter
create_listens: called with host '0.0.0.0' port '1234' family AF_INET(2)
getaddrinfo returned the following for host '0.0.0.0' port '1234'  family AF_INET
        cannonical name: '(nil)'
        flags: 1 family: AF_INET: socktype: SOCK_STREAM protocol IPPROTO_TCP addrlen 16
        sa_family: AF_INET sadata: 4 210 0 0 0 0 0 0 0 0 0 0 0 0 0 0
Starting netserver with host 'IN(6)ADDR_ANY' port '1234' and family AF_INET
accept_connections: enter
set_fdset: enter list 0x5f3130ac4740 fd_set 0x7fff92fb9450
setting 3 in fdset

```

使 `netserver` 监听在 1234 端口上，并指定 IPv4 协议，-D 表示不在后台运行。


- 启动 Client

```bash
netperf -H 127.0.0.1  -p 1234 -l 10  -t TCP_CRR -- -r 100,3000000
```

参数说明：
-H： 指定 server 的 IP 地址
-p： 指定 server 的 port
-l： 指定测试运行多长时间，单位：秒
-t： 运行模式，我们使用 `TCP_CRR` 来模拟 client 请求与 server 建连以后，由server 下发一定的数据量以后，关闭连接的这种 request/response 模式
-r： 分别指定 request 和 response 的字节数大小


### 默认配置Benchmark

记录下当前内核参数中与 tcp 相关的配置，并获取当前配置的 Benchmark 数据，用以在后续的优化中进行对比。
首先直接用 `netperf` 执行 20分钟的测试数据获取：

首先获取下当前测试流的关键帧的大小用以模拟尽量贴近实际业务场景的模拟：

```bash
# 首先 dump 到本地
ffmpeg -i http://xxxx/yyyyy/zzzzzzz.flv -c copy  1.flv

# 然后获取该片段的的首个关键帧的大小
ffmpeg -i 1.flv -frames:v 1 -f image2pipe -vcodec mjpeg - | wc -c

# 可以看到最后输出的为：117118，即 114KB 左右。
```

netperf 模拟 Player 请求下发直播数据，这里设置让 server 一次性下发 300KB 的数据，同时假设 Client 的 request 默认为 1KB,
测试 20分钟，命令如下：

**测试数据：**

```bash
$ ~$ netperf -H 127.0.0.1  -p 1234 -l 1200  -t TCP_CRR -- -r 1000,3000000
MIGRATED TCP Connect/Request/Response TEST from 0.0.0.0 (0.0.0.0) port 0 AF_INET to 127.0.0.1 () port 0 AF_INET : demo
Local /Remote
Socket Size   Request  Resp.   Elapsed  Trans.
Send   Recv   Size     Size    Time     Rate
bytes  Bytes  bytes    bytes   secs.    per sec

16384  131072 100      3000000  100.01      0.79
16384  131072
```

> 上述测试数据输出的 `Local /Remote` 可以看到在下面多出一行，分别表示的是 local 和 remote 的 socket send and recv buffer's bytes。
最后一列 `Trans` 表示的便是在测试的这段时间内平均每秒钟可以执行了多少次请求，即 0.79 次，相当于 Qps，越大说明效率越高。



## TCP 优化记录：

### 默认内核参数配置

```bash
[root@XXXLink64 ~]# sysctl -a | grep "net\.ipv4\.tcp"

net.ipv4.tcp_abort_on_overflow = 0
net.ipv4.tcp_adv_win_scale = 1
net.ipv4.tcp_allowed_congestion_control = reno cubic bbr
net.ipv4.tcp_app_win = 31
net.ipv4.tcp_autocorking = 1
net.ipv4.tcp_available_congestion_control = reno cubic bbr
net.ipv4.tcp_available_ulp = 
net.ipv4.tcp_base_mss = 1024
net.ipv4.tcp_challenge_ack_limit = 1000
net.ipv4.tcp_comp_sack_delay_ns = 1000000
net.ipv4.tcp_comp_sack_nr = 44
net.ipv4.tcp_congestion_control = cubic
net.ipv4.tcp_dsack = 1
net.ipv4.tcp_early_demux = 1
net.ipv4.tcp_early_retrans = 3
net.ipv4.tcp_ecn = 2
net.ipv4.tcp_ecn_fallback = 1
net.ipv4.tcp_fack = 0
net.ipv4.tcp_fastopen = 1
net.ipv4.tcp_fastopen_blackhole_timeout_sec = 3600
net.ipv4.tcp_fastopen_key = 5b1b3bb0-e9881f5a-8bf3fda0-1c410b36
net.ipv4.tcp_fin_timeout = 30
net.ipv4.tcp_frto = 2
net.ipv4.tcp_fwmark_accept = 0
net.ipv4.tcp_inherit_buffsize = 1
net.ipv4.tcp_init_cwnd = 15
net.ipv4.tcp_init_rto = 1000
net.ipv4.tcp_invalid_ratelimit = 500
net.ipv4.tcp_keepalive_intvl = 75
net.ipv4.tcp_keepalive_probes = 9
net.ipv4.tcp_keepalive_time = 7200
net.ipv4.tcp_l3mdev_accept = 0
net.ipv4.tcp_limit_output_bytes = 1048576
net.ipv4.tcp_loss_init_cwnd = 10
net.ipv4.tcp_low_latency = 0
net.ipv4.tcp_max_orphans = 524288
net.ipv4.tcp_max_reordering = 300
net.ipv4.tcp_max_syn_backlog = 62144
net.ipv4.tcp_max_tw_buckets = 6000
net.ipv4.tcp_mem = 2621440      3932160 5242880
net.ipv4.tcp_min_rtt_wlen = 300
net.ipv4.tcp_min_snd_mss = 48
net.ipv4.tcp_min_tso_segs = 2
net.ipv4.tcp_moderate_rcvbuf = 1
net.ipv4.tcp_mtu_probe_floor = 48
net.ipv4.tcp_mtu_probing = 0
net.ipv4.tcp_no_metrics_save = 1
net.ipv4.tcp_notsent_lowat = 8388608
net.ipv4.tcp_orphan_retries = 0
net.ipv4.tcp_pacing_ca_ratio = 120
net.ipv4.tcp_pacing_ss_ratio = 200
net.ipv4.tcp_probe_interval = 600
net.ipv4.tcp_probe_threshold = 8
net.ipv4.tcp_proc_sched = 1
net.ipv4.tcp_recovery = 1
net.ipv4.tcp_reordering = 3
net.ipv4.tcp_retrans_collapse = 1
net.ipv4.tcp_retries1 = 5
net.ipv4.tcp_retries2 = 15
net.ipv4.tcp_rfc1337 = 0
net.ipv4.tcp_rmem = 131072      1048576 16384000
net.ipv4.tcp_rto_max = 120
net.ipv4.tcp_rto_min = 200
net.ipv4.tcp_rx_skb_cache = 0
net.ipv4.tcp_sack = 1
net.ipv4.tcp_slow_start_after_idle = 0
net.ipv4.tcp_stdurg = 0
sysctl: net.ipv4.tcp_syn_retries = 2
net.ipv4.tcp_synack_retries = 2
reading key "net.ipv6.conf.all.stable_secret"net.ipv4.tcp_synack_rto_interval = 200

net.ipv4.tcp_syncookies = 1
net.ipv4.tcp_thin_linear_timeouts = 0
net.ipv4.tcp_timestamps = 0
net.ipv4.tcp_tso_win_divisor = 3
net.ipv4.tcp_tw_ignore_syn_tsval_zero = 1
net.ipv4.tcp_tw_reuse = 1
net.ipv4.tcp_tw_timeout = 60
net.ipv4.tcp_tx_skb_cache = 0
net.ipv4.tcp_wan_timestamps = 0
net.ipv4.tcp_window_scaling = 1
net.ipv4.tcp_wmem = 4096000     16384000        32768000
net.ipv4.tcp_workaround_signed_windows = 0
```

### 优化内容

以下均为仅优化单边的 Server 测参数。

编辑 `/etc/sysctl.conf`，添加或修改如下参数：

```bash
# 设置 tcp 拥塞算法为 bbr
net.ipv4.tcp_congestion_control = bbr

```

使参数立即生效：

```bash
$ sysctl -p
```

#### 测试结果

```bash
$ ~$ netperf -H 127.0.0.1  -p 1234 -l 300  -t TCP_CRR -- -r 1000,3000000
MIGRATED TCP Connect/Request/Response TEST from 0.0.0.0 (0.0.0.0) port 0 AF_INET to 127.0.0.1 () port 0 AF_INET : demo
Local /Remote
Socket Size   Request  Resp.   Elapsed  Trans.
Send   Recv   Size     Size    Time     Rate
bytes  Bytes  bytes    bytes   secs.    per sec

436600 87380  100      3000000  300.00      1.06
436600 87380
```

可以看到在开启了 bbr 拥塞算法后，`Trans` 由 0.79 升到了 1.06，有了明显的提升。
但是实际优化数据不会这么明显，因为线上环境我们只能开启 server 测的 bbr，而无法控制 client 同时开启。
